# -*- coding: utf-8 -*-
"""final pyml project of accidental deaths in india

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1G-zgDZCNR_gz3PYv_ipddt-aqPleGq-B

# Route Risk – ML Forecasting of Accidental Deaths in India

## Objective
The aim of this project is to build an intelligent machine learning application that predicts the risk level of traffic-related deaths based on accident statistics in India.  
The model helps identify **Low**, **Medium**, or **High** accident death risk levels for different regions, supporting better public safety, infrastructure, and transportation planning.

---

### Dataset
- **Source:** ADSI (Accidental Deaths & Suicides in India) – Ministry of Road Transport & Highways, Government of India  
- **Key Features:**  
  - Road / Railway / Crossing accident counts, injuries, and deaths  
  - Total traffic accident statistics  

---
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

import numpy as np


import matplotlib.pyplot as plt
import seaborn as sns


from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

"""## Step 1 – Exploratory Data Analysis (EDA)
In this section, I explored patterns and trends in the dataset:
- Accident distribution across states
- Comparison between road, railway, and crossing accidents
- Visualization of injuries vs. deaths
These insights guided feature selection and model building.

"""

df=pd.read_csv('/content/ADSI_Table_1A.2 (1).csv')

df.head()

print("Dataset Shape:", df.shape)

print("\nDataset Info:")
print(df.info())

print("\nMissing Values:")
print(df.isnull().sum())

# Check for missing values
print("Missing values before cleaning:")
print(df.isnull().sum())

# Option 1: Drop rows with missing values
df = df.dropna()

# Option 2 (alternative): Fill missing values with a default or mean
# df['ColumnName'] = df['ColumnName'].fillna(df['ColumnName'].mean())

print("Missing values after cleaning:")
print(df.isnull().sum())

print("Duplicates before:", df.duplicated().sum())
df = df.drop_duplicates()
print("Duplicates after:", df.duplicated().sum())

print("\nStatistical summary:")
print(df.describe())

print("\nData types and non-null counts:")
print(df.info())

print("Shape of dataset:", df.shape)

print("\nUnique values in categorical columns:")
for col in df.select_dtypes(include=['object']).columns:
    print(f"{col}: {df[col].nunique()} unique values")

import matplotlib.pyplot as plt
import seaborn as sns

# Set style
sns.set(style="whitegrid")

# Plot histograms for all numeric columns
numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns

plt.figure(figsize=(15, 10))
for i, col in enumerate(numeric_cols, 1):
    plt.subplot((len(numeric_cols) // 3) + 1, 3, i)
    sns.histplot(df[col], bins=20, kde=True, color='skyblue')
    plt.title(f"Distribution of {col}")
plt.tight_layout()
plt.show()

# Group and sort for injuries
top_injuries = df.groupby('State/UT/City')['Total Traffic Accidents - Injured'].sum().sort_values(ascending=False).head(10)

# Plot
plt.figure(figsize=(10, 6))
top_injuries.plot(kind='barh', color='orange')
plt.title("Top 10 States/UTs by Traffic Accident Injuries")
plt.xlabel("Injuries")
plt.ylabel("State/UT/City")
plt.gca().invert_yaxis()  # Highest at top
plt.show()

df.columns

import numpy as np
import matplotlib.pyplot as plt

# Group, clean, sort, and take Top 10
state_deaths = (
    df.groupby('State/UT/City')['Total Traffic Accidents - Died']
      .sum()
      .fillna(0)
      .astype(float)
      .sort_values(ascending=False)
      .head(10)
)

# Plot horizontal bar with value labels
plt.figure(figsize=(10, 6))
ax = state_deaths.sort_values().plot(kind='barh')
ax.set_title("Top 10 States/UTs by Traffic Accident Deaths")
ax.set_xlabel("Deaths")
ax.set_ylabel("State/UT/City")

# Add value labels at the end of bars
for p in ax.patches:
    width = p.get_width()
    ax.text(width + max(state_deaths)*0.01, p.get_y() + p.get_height()/2,
            f"{int(width):,}", va='center', fontsize=9)

plt.tight_layout()
plt.show()

road = df['Road Accidents - Died'].sum()
rail = df['Railway Accidents - Died'].sum()
cross = df['Railway Crossing Accidents - Died'].sum()

plt.figure(figsize=(6,6))
plt.pie([road, rail, cross],
        labels=['Road', 'Railway', 'Railway Crossing'],
        autopct='%1.1f%%', startangle=90)
plt.title("Share of Deaths by Accident Type")
plt.tight_layout()
plt.show()

# Standardize column names (simple, optional)
df.columns = [c.strip() for c in df.columns]

# Ensure numeric columns are numeric
num_cols = [
    'Total Traffic Accidents - Cases','Total Traffic Accidents - Injured','Total Traffic Accidents - Died',
    'Road Accidents - Cases','Road Accidents - Injured','Road Accidents - Died',
    'Railway Accidents - Cases','Railway Accidents - Injured','Railway Accidents - Died',
    'Railway Crossing Accidents - Cases','Railway Crossing Accidents - Injured','Railway Crossing Accidents - Died'
]
for c in num_cols:
    if c in df.columns:
        df[c] = pd.to_numeric(df[c], errors='coerce')

# Drop duplicates & obvious empty rows
df = df.drop_duplicates().dropna(subset=['State/UT/City']).reset_index(drop=True)

# Safe division helper
def safe_divide(a, b):
    import numpy as np
    return np.where(b==0, 0, a/b)

df['FatalityRate_Total']   = safe_divide(df['Total Traffic Accidents - Died'],   df['Total Traffic Accidents - Cases'])
df['FatalityRate_Road']    = safe_divide(df['Road Accidents - Died'],            df['Road Accidents - Cases'])
df['FatalityRate_Railway'] = safe_divide(df['Railway Accidents - Died'],         df['Railway Accidents - Cases'])
df['FatalityRate_Cross']   = safe_divide(df['Railway Crossing Accidents - Died'],df['Railway Crossing Accidents - Cases'])

import numpy as np
import matplotlib.pyplot as plt

top10_idx = (
    df.groupby('State/UT/City')['Total Traffic Accidents - Died']
      .sum().sort_values(ascending=False).head(10).index
)
agg = df.groupby('State/UT/City')[['Total Traffic Accidents - Died','Total Traffic Accidents - Injured']].sum().loc[top10_idx]

x = np.arange(len(agg)); w = 0.38
plt.figure(figsize=(12,6))
plt.bar(x - w/2, agg['Total Traffic Accidents - Died'], width=w, label='Deaths')
plt.bar(x + w/2, agg['Total Traffic Accidents - Injured'], width=w, label='Injured')
plt.xticks(x, agg.index, rotation=45, ha='right'); plt.ylabel("Count")
plt.title("Top 10 States/UTs: Deaths vs Injured"); plt.legend(); plt.tight_layout(); plt.show()

fat = (df.groupby('State/UT/City')[['Total Traffic Accidents - Died','Total Traffic Accidents - Cases']]
         .sum())
fat['FatalityRate_Total'] = safe_divide(fat['Total Traffic Accidents - Died'], fat['Total Traffic Accidents - Cases'])
top_rate = fat[fat['Total Traffic Accidents - Cases']>0]['FatalityRate_Total'].sort_values(ascending=False).head(10)

plt.figure(figsize=(10,6))
top_rate.sort_values().plot(kind='barh')
plt.xlabel("Fatality Rate (Deaths / Cases)"); plt.ylabel("State/UT/City")
plt.title("Top 10 States/UTs by Fatality Rate"); plt.tight_layout(); plt.show()

# Top 5 states by total cases
top_cases = df.groupby('State/UT/City')['Total Traffic Accidents - Cases'].sum().sort_values(ascending=False).head(5)

# Top 5 states by injuries
top_injuries = df.groupby('State/UT/City')['Total Traffic Accidents - Injured'].sum().sort_values(ascending=False).head(5)

# Plot side-by-side
fig, axes = plt.subplots(1, 2, figsize=(14, 6))
top_cases.plot(kind='bar', ax=axes[0], color='skyblue', title='Top 5 States by Accident Cases')
axes[0].set_ylabel("Cases")
top_injuries.plot(kind='bar', ax=axes[1], color='salmon', title='Top 5 States by Accident Injuries')
axes[1].set_ylabel("Injuries")
plt.tight_layout()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# Sort by FatalityRate_Total and select top 10 states
top_states = df.sort_values(by='FatalityRate_Total', ascending=False).head(10)

# Create grouped bar chart
plt.figure(figsize=(14, 7))
bar_width = 0.2
x = range(len(top_states))

plt.bar([i - 1.5*bar_width for i in x], top_states['FatalityRate_Total'], width=bar_width, label='Total Fatality Rate')
plt.bar([i - 0.5*bar_width for i in x], top_states['FatalityRate_Road'], width=bar_width, label='Road Fatality Rate')
plt.bar([i + 0.5*bar_width for i in x], top_states['FatalityRate_Railway'], width=bar_width, label='Railway Fatality Rate')
plt.bar([i + 1.5*bar_width for i in x], top_states['FatalityRate_Cross'], width=bar_width, label='Railway Crossing Fatality Rate')

# Labels & formatting
plt.xticks(x, top_states['State/UT/City'], rotation=45, ha='right')
plt.xlabel("State/UT/City")
plt.ylabel("Fatality Rate (%)")
plt.title("Fatality Rate Comparison for Top 10 States (by Total Fatality Rate)")
plt.legend()
plt.tight_layout()
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

# Select only numerical columns for correlation
numeric_cols = df.select_dtypes(include=['float64', 'int64'])

# Compute correlation
corr_matrix = numeric_cols.corr()

# Plot heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title("Correlation Heatmap of Accident Data", fontsize=16)
plt.show()

import matplotlib.pyplot as plt

death_types = [
    "Road Accidents - Died",
    "Railway Accidents - Died",
    "Railway Crossing Accidents - Died"
]

# Filter states with any deaths > 0
df_deaths = df[["State/UT/City"] + death_types]
df_deaths = df_deaths[(df_deaths[death_types].sum(axis=1) > 0)]

# Plot stacked bar chart
df_deaths.set_index("State/UT/City")[death_types].plot(kind="bar", stacked=True, figsize=(14, 7))
plt.title("Deaths by Accident Type per State/UT/City")
plt.ylabel("Number of Deaths")
plt.xlabel("State/UT/City")
plt.xticks(rotation=90)
plt.legend(title="Accident Type")
plt.tight_layout()
plt.show()

# Boxplot for Total Traffic Accidents - Died
plt.figure(figsize=(8, 5))
sns.boxplot(x=df["Total Traffic Accidents - Died"])
plt.title("Boxplot - Total Traffic Accidents Deaths")
plt.show()

# Boxplot for Road Accidents - Died
plt.figure(figsize=(8, 5))
sns.boxplot(x=df["Road Accidents - Died"])
plt.title("Boxplot - Road Accidents Deaths")
plt.show()

# Boxplot to visually identify outliers
plt.figure(figsize=(8,6))
plt.boxplot(df['Total Traffic Accidents - Died'])
plt.title("Outlier Detection - Deaths")
plt.ylabel("Deaths")
plt.show()

# Get actual outlier states (simple IQR method)
Q1 = df['Total Traffic Accidents - Died'].quantile(0.25)
Q3 = df['Total Traffic Accidents - Died'].quantile(0.75)
IQR = Q3 - Q1
outliers = df[df['Total Traffic Accidents - Died'] > (Q3 + 1.5 * IQR)]['State/UT/City']

print("States with unusually high deaths:")
print(outliers.values)

def risk_level(x):
    if x < 50:
        return "Low"
    elif x < 150:
        return "Medium"
    else:
        return "High"

df['Risk_Level'] = df['Total Traffic Accidents - Died'].apply(risk_level)

"""## Step 2 – Model Building
I tested multiple supervised learning algorithms:
1. Logistic Regression  
2. Decision Tree  
3. Random Forest  

Random Forest was chosen because:
- It had the highest accuracy during testing.
- It generalizes well to unseen data.
- It handles mixed numerical features effectively.

"""

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, f1_score, confusion_matrix



df['Risk_Level'] = df['Total Traffic Accidents - Died'].apply(risk_level)

# --- Features & Target ---
X = df[['Road Accidents - Cases', 'Road Accidents - Injured', 'Road Accidents - Died',
        'Railway Accidents - Cases', 'Railway Accidents - Injured', 'Railway Accidents - Died',
        'Railway Crossing Accidents - Cases', 'Railway Crossing Accidents - Injured', 'Railway Crossing Accidents - Died']]

y = df['Risk_Level']

# --- Train-Test Split ---
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ===== Random Forest =====
rf_model = RandomForestClassifier(random_state=42)
rf_model.fit(X_train, y_train)
rf_pred = rf_model.predict(X_test)

print("=== Random Forest Results ===")
print("Accuracy:", accuracy_score(y_test, rf_pred))
print("F1 Score:", f1_score(y_test, rf_pred, average='macro'))
print(classification_report(y_test, rf_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, rf_pred))

# ===== Logistic Regression =====
lr_model = LogisticRegression(max_iter=1000, random_state=42)
lr_model.fit(X_train, y_train)
lr_pred = lr_model.predict(X_test)

print("\n=== Logistic Regression Results ===")
print("Accuracy:", accuracy_score(y_test, lr_pred))
print("F1 Score:", f1_score(y_test, lr_pred, average='macro'))
print(classification_report(y_test, lr_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, lr_pred))

import pandas as pd

# Create performance summary
results_df = pd.DataFrame({
    'Model': ['Random Forest', 'Logistic Regression'],
    'Accuracy': [
        accuracy_score(y_test, rf_pred),
        accuracy_score(y_test, lr_pred)
    ],
    'F1 Score (macro)': [
        f1_score(y_test, rf_pred, average='macro'),
        f1_score(y_test, lr_pred, average='macro')
    ]
})

print("=== Model Comparison ===")
print(results_df)

import matplotlib.pyplot as plt
import numpy as np

# Get feature importances
importances = rf_model.feature_importances_
indices = np.argsort(importances)[::-1]
feature_names = X.columns

# Plot
plt.figure(figsize=(8, 5))
plt.bar(range(len(importances)), importances[indices], align='center')
plt.xticks(range(len(importances)), feature_names[indices], rotation=45, ha='right')
plt.title("Random Forest Feature Importance")
plt.tight_layout()
plt.show()

"""## Insights from the Analysis
1. Maharashtra has the highest total traffic accident deaths.
2. Road accidents contribute the majority of deaths compared to railway and crossing accidents.
3. The top 5 states account for more than 50% of total accident deaths.
4. Injuries are proportionally high in states with larger populations and higher vehicle density.
5. Correlation heatmap shows a strong link between number of cases and number of deaths.
"""

rf_acc = accuracy_score(y_test, rf_pred) * 100
rf_f1 = f1_score(y_test, rf_pred, average='macro')
lr_acc = accuracy_score(y_test, lr_pred) * 100
lr_f1 = f1_score(y_test, lr_pred, average='macro')

print(rf_acc, rf_f1, lr_acc, lr_f1)  # to confirm values

from IPython.display import Markdown

rf_acc = accuracy_score(y_test, rf_pred) * 100
rf_f1 = f1_score(y_test, rf_pred, average='macro')
lr_acc = accuracy_score(y_test, lr_pred) * 100
lr_f1 = f1_score(y_test, lr_pred, average='macro')

display(Markdown(f"""
## Conclusion

This project analyzed accidental deaths in India with the aim of predicting **risk levels** based on accident statistics from various sources - road, railway, and railway crossing incidents.

Two machine learning models were developed and evaluated:

- **Random Forest**
  - Accuracy: **{rf_acc:.2f}%**
  - F1 Score (macro): **{rf_f1:.2f}**

- **Logistic Regression**
  - Accuracy: **{lr_acc:.2f}%**
  - F1 Score (macro): **{lr_f1:.2f}**

Random Forest outperformed Logistic Regression, indicating its strength in capturing **non-linear relationships** and **feature interactions** within the dataset.

**Key Influential Features** identified from the Random Forest model:
1. Road Accidents – Died
2. Railway Accidents – Died
3. Road Accidents – Injured

These results can guide policymakers and road safety authorities to **prioritize safety measures** in areas with high fatality counts, especially in road accidents.
"""))